pr_number,model,timestamp,prompt,risk_type_labels,explanations,raw_response,error,success
9115,Mistral,1769332134.9472132,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""Bump dev version to 2.6.1-SNAPSHOT"",
    ""pr_description"": ""As part of the 2.6.0 release, we need to bump the dev version of the 2.6 branch to 2.6.1.""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No SZZ origin issues exist for this pull request.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No SZZ origin issues exist for this pull request.""}]}",,True
4403,Mistral,1769332140.983773,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""Fix Streams navigation, menu order, and missig maven content"",
    ""pr_description"": ""Fix navigation\nUpdate libraries and maven topics""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No SZZ origin issues were found for this pull request, and there is no observable failure or reported issue attributed to it.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No SZZ origin issues were found for this pull request, and there is no observable failure or reported issue attributed to it.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
14431,Mistral,1769332148.2103775,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-9366"",
      ""title"": ""Upgrade log4j to log4j2"",
      ""priority"": ""Critical"",
      ""description"": ""h2. CVE-2019-17571\u00a0Detail\r\n\r\nIncluded in Log4j 1.2 is a SocketServer class that is vulnerable to deserialization of untrusted data which can be exploited to remotely execute arbitrary code when combined with a deserialization gadget when listening to untrusted network traffic for log data. This affects Log4j versions up to 1.2 up to 1.2.17.\r\n\r\n\u00a0\r\n\r\n[https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-17571]\r\n\r\n\u00a0""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-15485: Upgrade gradle, plugins and test libraries (2/N)"",
    ""pr_description"": ""To prepare Java 21 support, upgrade gradle, its plugins and test libraries.\nRelease notes for major and minor updates included below.\nThe highlight is faster Java compilation by not shutting down the daemon at the end of the build.\nGradle's internal performance tests show up to a 30% build time improvement for builds that are\ndominated by compiling Java sources.\nMockito turns out to be a complex case where we use one of 3 different versions depending on\nthe Scala and Java versions used. In addition, the default mocking strategy changed from subclass\nto inline in Mockito 5.0. We now use inline across the board (we previously used both subclass\nand inline). See comments in the relevant parts of the code for more details.\n\nGradle 8.3 release notes: https://docs.gradle.org/8.3/release-notes.html\njmh 1.37: virtual thread support and various bug fixes\nJUnit 5.10.0 release notes: https://junit.org/junit5/docs/5.10.0/release-notes/index.html\nMockito 5.x release notes:\n\nhttps://github.com/mockito/mockito/releases/tag/v5.0.0\nhttps://github.com/mockito/mockito/releases/tag/v5.1.0\nhttps://github.com/mockito/mockito/releases/tag/v5.2.0\nhttps://github.com/mockito/mockito/releases/tag/v5.3.0\nhttps://github.com/mockito/mockito/releases/tag/v5.4.0\nhttps://github.com/mockito/mockito/releases/tag/v5.5.0\n\n\nEasyMock 5.2.0 release notes: https://github.com/easymock/easymock/releases/tag/easymock-5.2.0\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Security Risk'],"[{'label': 'Security Risk', 'confidence': 1.0, 'rationale': ""The SZZ origin issue 'KAFKA-9366' describes a critical vulnerability (CVE-2019-17571) in Log4j that can lead to remote code execution when combined with a deserialization gadget. This vulnerability is introduced by the PR 'KAFKA-15485', which upgrades log4j to log4j2.""}]","{""risk_type_labels"": [""Security Risk""], ""explanations"": [{""label"": ""Security Risk"", ""confidence"": 1.0, ""rationale"": ""The SZZ origin issue 'KAFKA-9366' describes a critical vulnerability (CVE-2019-17571) in Log4j that can lead to remote code execution when combined with a deserialization gadget. This vulnerability is introduced by the PR 'KAFKA-15485', which upgrades log4j to log4j2.""}], ""IMPORTANT"": ""The output is valid JSON.""}",,True
10582,Mistral,1769332152.8912814,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Bump latest 2.6 version to 2.6.2"",
    ""pr_description"": NaN
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'There are no SZZ origin issues linked to this PR.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""There are no SZZ origin issues linked to this PR.""}]}",,True
9551,Mistral,1769332159.382948,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-13430"",
      ""title"": ""Remove broker-wide quota properties from the documentation"",
      ""priority"": ""Major"",
      ""description"": ""I found this problem while working on [KAFKA-13341|https://issues.apache.org/jira/browse/KAFKA-13341].\r\n\r\nBroker-wide quota properties ({{quota.producer.default}}, {{quota.consumer.default}}) are [removed in 3.0|https://issues.apache.org/jira/browse/KAFKA-12591], but it is not applied to the documentation yet.""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-10679: Migrate upgrade changes from site to kafka/docs"",
    ""pr_description"": ""During the AK website upgrade, changes made to kafka-site weren't migrated back to kafka-docs.\nThis PR is an initial attempt at porting the changes to kafka/docs, but it does not include the streams changes. Those will come in a separate PR.\nFor the most part, the bulk of the changes in the PR are cosmetic.  Only the introduction.html has substantial changes, but it's a direct port from the live documentation.\nFor testing:\n\nI reviewed the PR diffs\nRendered the changes locally\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Bug Risk'],"[{'label': 'Bug Risk', 'confidence': 0.9, 'rationale': ""The SZZ origin issue 'KAFKA-13430' points to an incomplete migration of broker-wide quota properties from kafka-site to kafka/docs, as noted by the absence of these changes in the PR description.""}]","{""risk_type_labels"": [""Bug Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 0.9, ""rationale"": ""The SZZ origin issue 'KAFKA-13430' points to an incomplete migration of broker-wide quota properties from kafka-site to kafka/docs, as noted by the absence of these changes in the PR description.""}], ""IMPORTANT"": ""The output is valid JSON""}",,True
12424,Mistral,1769332165.8910582,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""RELEASE: Add 3.2 upgrade docs"",
    ""pr_description"": ""Looking through the issues fixed in https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.2.1, I didn't see any changes to public APIs (config/metrics/CLI/etc) or any changes to default behaviors. I picked three major issues to include in the release notes.\n\nhttps://issues.apache.org/jira/browse/KAFKA-14062 OAuth refresh problem -- the driver for this release\nhttps://issues.apache.org/jira/browse/KAFKA-14079 a major Connect OOM issue\nhttps://issues.apache.org/jira/browse/KAFKA-14024 3.2.0 consumer regression""
  }
}",[],"[{'label': 'Non-risky', 'confidence': 1, 'rationale': 'The pull request description does not mention any observable failures or SZZ origin issues related to functional, logical, security, performance, or maintainability problems. The only mentioned issues are related to the release notes and do not provide evidence of empirically observed failures.'}]","{""risk_type_labels"": [], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1, ""rationale"": ""The pull request description does not mention any observable failures or SZZ origin issues related to functional, logical, security, performance, or maintainability problems. The only mentioned issues are related to the release notes and do not provide evidence of empirically observed failures.""}], ""IMPORTANT"": ""The PR is deemed 'Non-risky' as no SZZ origin issues were found and there is no mention of any observable failures in the provided pull request details.""}",,True
12564,Mistral,1769332171.747758,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Cleanups from KAFKA-14097"",
    ""pr_description"": ""As per Luke's comments in #12501, I added the correct name and removed the hard coded values for the config in tests.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'There are no SZZ origin issues linked to this pull request, and there is no observable failure evidence in the PR title or description.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""There are no SZZ origin issues linked to this pull request, and there is no observable failure evidence in the PR title or description.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
12661,Mistral,1769332176.3603287,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Update release versions for upgrade tests with 3.0.2, 3.1.2, 3.2.3 release"",
    ""pr_description"": ""Updates release versions in files that are used for upgrade test with the 3.0.2, 3.1.2, 3.2.3 release version.""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No SZZ origin issues were found related to this pull request.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No SZZ origin issues were found related to this pull request.""}]}",,True
4729,Mistral,1769332181.981348,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-6676: Ensure Kafka chroot exists in system tests and use chroot on one test with security parameterizations"",
    ""pr_description"": ""Ensures Kafka chroot exists in ZK when starting KafkaService so commands that use ZK and are executed before the first Kafka broker starts do not fail due to the missing chroot.\nAlso uses chroot with one test that also has security parameterizations so Kafka's test suite exercises these combinations. Previously no tests were exercising chroots.\nTo validate, I kicked off a test run of the sanity_checks (which include the chroot-ed test as well as some non-chroot-ed tests).\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No observable failures or SZZ origin issues have been reported in the provided context.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No observable failures or SZZ origin issues have been reported in the provided context.""}], ""IMPORTANT"": ""The 'confidence' field MUST be a numeric value, not a string.""}",,True
5230,Mistral,1769332192.6399877,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-17219"",
      ""title"": ""Adjust system test framework for new protocol consumer"",
      ""priority"": ""Blocker"",
      ""description"": ""The current test framework doesn't work well with the existing tests using the new consumer protocol. There are two main issues I've seen.\r\n\u00a0\r\nFirst, we sometimes assume there is no rebalance triggered, for instance in {{consumer_test.py::test_consumer_failure}}\r\n{code:java}\r\nverify that there were no rebalances on failover\r\nassert num_rebalances == consumer.num_rebalances(), \""Broker failure should not cause a rebalance\""{code}\r\nThe current frame work calculates {{num_rebalances}}\u00a0by increment by one every time a new assignment is received, so if a reconciliation happened during the failover,\u00a0{{num_rebalances}}\u00a0will also be incremented. For new protocol we need a new way to update\u00a0{{{}num_rebalances{}}}.\r\n\u00a0\r\nSecond, for the new protocol, we need a way to make sure all members have joined {*}and stablized{*}. Currently we only make sure all members have joined (the event handlers are all in Joined state), where some partitions haven't been assigned and more time is needed for reconciliation. The issue can cause failure in assertions like timeout waiting for consumption and\r\n{code:java}\r\npartition_owner = consumer.owner(partition)\r\nassert partition_owner is not None {code}\r\n\u00a0\r\nFor a short term solution, we can make the tests pass by bypassing with adding {{time.sleep}}\u00a0or skip checking\u00a0{{{}num_rebalance{}}}. To truly fix them, we should adjust\u00a0{{tools/src/main/java/org/apache/kafka/tools/VerifiableConsumer.java}}\u00a0to work well with the new protocol.""
    },
    {
      ""issue_key"": ""KAFKA-7920"",
      ""title"": ""Do not permit zstd use until inter.broker.protocol.version is updated to 2.1"",
      ""priority"": ""Blocker"",
      ""description"": ""After brokers have been upgraded to 2.1, users can begin using zstd compression. Regardless of the inter.broker.protocol.version, the broker will happily accept zstd-compressed data as long as the right produce request version is used. However, if the inter.broker.protocol.version is set to 2.0 or below, then followers will not be able to use the minimum required fetch version, which will result in the following error:\r\n\r\n{code}\r\n[2019-02-11 17:42:47,116] ERROR [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Error for partition foo-0 at offset 0 (kafka.server.ReplicaFetcherThread)                                                                                                                          \r\norg.apache.kafka.common.errors.UnsupportedCompressionTypeException: The requesting client does not support the compression type of given partition.\r\n{code}\r\n\r\nWe should make produce request validation consistent. Until the inter.broker.protocol.version is at 2.1 or later, we should reject produce requests with UNSUPPORTED_COMPRESSION_TYPE.""
    },
    {
      ""issue_key"": ""KAFKA-7965"",
      ""title"": ""Flaky Test ConsumerBounceTest#testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup"",
      ""priority"": ""Critical"",
      ""description"": ""To get stable nightly builds for `2.2` release, I create tickets for all observed test failures.\r\n\r\n[https://jenkins.confluent.io/job/apache-kafka-test/job/2.2/21/]\r\n{quote}java.lang.AssertionError: Received 0, expected at least 68 at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.assertTrue(Assert.java:41) at kafka.api.ConsumerBounceTest.receiveAndCommit(ConsumerBounceTest.scala:557) at kafka.api.ConsumerBounceTest.$anonfun$testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup$1(ConsumerBounceTest.scala:320) at kafka.api.ConsumerBounceTest.$anonfun$testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup$1$adapted(ConsumerBounceTest.scala:319) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at kafka.api.ConsumerBounceTest.testRollingBrokerRestartsWithSmallerMaxGroupSizeConfigDisruptsBigGroup(ConsumerBounceTest.scala:319){quote}""
    },
    {
      ""issue_key"": ""KAFKA-17434"",
      ""title"": ""upgrade_test.py tests impossible upgrade scenarios"",
      ""priority"": ""Major"",
      ""description"": ""Because of KIP-902 (Upgrade Zookeeper version to 3.8.2), it is not possible to upgrade from a Kafka version earlier than 3.5 to a version later than 3.5. Therefore, we should not test these upgrade scenarios in upgrade_test.py. They do happen to work sometimes, but only in the trivial case where we don't create topics or make changes during the upgrade (which would reveal the ZK incompatibility).""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-2983: Remove Scala consumers and related code"",
    ""pr_description"": ""Removed Scala consumers (SimpleConsumer and ZooKeeperConsumerConnector)\nand their tests.\nRemoved Scala request/response/message classes.\nRemoved any mention of new consumer or new producer in the code\nwith the exception of MirrorMaker where the new.consumer option was\nnever deprecated so we have to keep it for now. The non-code\ndocumentation has not been updated either, that will be done\nseparately.\nRemoved a number of tools that only made sense in the context\nof the Scala consumers (see upgrade notes).\nUpdated some tools that worked with both Scala and Java consumers\nso that they only support the latter (see upgrade notes).\nRemoved BaseConsumer and related classes apart from BaseRecord\nwhich is used in MirrorMakerMessageHandler. The latter is a pluggable\ninterface so effectively public API.\nRemoved ZkUtils methods that were only used by the old consumers.\nRemoved ZkUtils.registerBroker and ZKCheckedEphemeral since\nthe broker now uses the methods in KafkaZkClient and no-one else\nshould be using that method.\nUpdated system tests so that they don't use the Scala consumers except\nfor multi-version tests.\nUpdated LogDirFailureTest so that the consumer offsets topic would\ncontinue to be available after all the failures. This was necessary for it\nto work with the Java consumer.\nSome multi-version system tests had not been updated to include\nrecently released Kafka versions, fixed it.\nUpdated findBugs and checkstyle configs not to refer to deleted\nclasses and packages.\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}","['Bug Risk', 'Security Risk', 'Maintainability Risk']","[{'label': 'Bug Risk', 'confidence': 1.0, 'rationale': 'Issue KAFKA-17219 mentions incorrect behavior and failures in assertions due to changes in the test framework for the new protocol consumer.'}, {'label': 'Security Risk', 'confidence': 1.0, 'rationale': 'Issue KAFKA-7920 describes a situation where brokers accept zstd-compressed data even when the inter.broker.protocol.version is set to 2.0 or below, leading to an UnsupportedCompressionTypeException.'}, {'label': 'Maintainability Risk', 'confidence': 1.0, 'rationale': 'Issue KAFKA-7965 concerns a flaky test that disrupts big groups during rolling broker restarts, and it is mentioned that these tests should be updated for stable nightly builds.'}]","{""risk_type_labels"": [""Bug Risk"", ""Security Risk"", ""Maintainability Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 1.0, ""rationale"": ""Issue KAFKA-17219 mentions incorrect behavior and failures in assertions due to changes in the test framework for the new protocol consumer.""}, {""label"": ""Security Risk"", ""confidence"": 1.0, ""rationale"": ""Issue KAFKA-7920 describes a situation where brokers accept zstd-compressed data even when the inter.broker.protocol.version is set to 2.0 or below, leading to an UnsupportedCompressionTypeException.""}, {""label"": ""Maintainability Risk"", ""confidence"": 1.0, ""rationale"": ""Issue KAFKA-7965 concerns a flaky test that disrupts big groups during rolling broker restarts, and it is mentioned that these tests should be updated for stable nightly builds.""}]}",,True
5137,Mistral,1769332197.5938797,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Rename package `internal` to `internals` for consistency"",
    ""pr_description"": ""Committer Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No SZZ origin issues have been reported for this PR. Therefore, it is non-risky according to the defined principles.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No SZZ origin issues have been reported for this PR. Therefore, it is non-risky according to the defined principles.""}]}",,True
14486,Mistral,1769332202.6469712,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Update the list of collaborators"",
    ""pr_description"": ""The collaborators list was initially added in #13713 (following the policy specified here - apache/kafka-site#510).\nThe list is currently outdated since 4 of the 10 collaborators are now committers.\nUntil https://issues.apache.org/jira/browse/KAFKA-14995 is resolved, this list needs to be maintained manually.\nThe list has been updated here following the same methodology outlined originally:\n\n> git shortlog --numbered --summary --since=\""1 year ago\""\n\n    76\tDavid Jacot\n    62\tGreg Harris\n    47\tDavid Arthur\n    46\tYash Mayya\n    44\tIsmael Juma\n    43\tLuke Chen\n    41\tLucas Brutschy\n    37\tColin Patrick McCabe\n    35\tVictoria Xia\n    34\tJos\u00e9 Armando Garc\u00eda Sancio\n    33\tChris Egerton\n    30\tMickael Maison\n    29\tChristo Lolov\n    29\tDivij Vaidya\n    27\tFederico Valeri\n    27\tPhilip Nee\n    26\tvamossagar12\n    24\tChia-Ping Tsai\n    23\tKamal Chandraprakash\n    20\tJustine Olshan\n    20\tMatthias J. Sax\n    20\tSatish Duggana\n    19\thudeqi\n    17\tA. Sophie Blee-Goldman\n    15\tHao Li\n    15\tJeff Kim\n    14\tGantigmaa Selenge\n    14\tJason Gustafson\n    13\tBruno Cadonna\n    13\tKirk True\n    12\tRon Dagostino\n    11\tGuozhang Wang\n    11\tManyanda Chitimbo\n    11\tProven Provenzano\n    11\tandymg3\n    10\tSaid Boudjelda\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'There are no SZZ origin issues found for this pull request, indicating that there are no observable failures attributed to it.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""There are no SZZ origin issues found for this pull request, indicating that there are no observable failures attributed to it.""}]}",,True
6648,Mistral,1769332208.014483,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-3061"",
      ""title"": ""Get rid of Guava dependency"",
      ""priority"": ""Major"",
      ""description"": ""KAFKA-2422 adds Reflections library to KafkaConnect, which depends on Guava.\nSince lots of people want to use Guavas, having it in the framework will lead to conflicts.""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Gradle and Gradle plugins versions are extracted into 'gradle/dependencies.gradle' file"",
    ""pr_description"": ""Committer Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Bug Risk'],"[{'label': 'Bug Risk', 'confidence': 1.0, 'rationale': 'The SZZ origin issue KAFKA-3061 describes the addition of Reflections library to KafkaConnect which introduces a dependency on Guava, potentially leading to conflicts. This constitutes a functional failure and falls under the Bug Risk category.'}]","{""risk_type_labels"": [""Bug Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 1.0, ""rationale"": ""The SZZ origin issue KAFKA-3061 describes the addition of Reflections library to KafkaConnect which introduces a dependency on Guava, potentially leading to conflicts. This constitutes a functional failure and falls under the Bug Risk category.""}]}",,True
14411,Mistral,1769332214.4523597,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Used Admin instead of AdminClient to create Admin"",
    ""pr_description"": ""Used Admin instead of AdminClient to create Admin\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Bug Risk'],"[{'label': 'Bug Risk', 'confidence': 1.0, 'rationale': ""The PR modifies the usage of 'Admin' to 'AdminClient', which may introduce unexpected behavior or errors (e.g., incorrect method calls, missing functionality). This change is not directly supported in the PR description, but it violates expected coding standards and could lead to functional issues.""}]","{""risk_type_labels"": [""Bug Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 1.0, ""rationale"": ""The PR modifies the usage of 'Admin' to 'AdminClient', which may introduce unexpected behavior or errors (e.g., incorrect method calls, missing functionality). This change is not directly supported in the PR description, but it violates expected coding standards and could lead to functional issues.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
6884,Mistral,1769332256.6359053,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-10134"",
      ""title"": ""High CPU issue during rebalance in Kafka consumer after upgrading to 2.5"",
      ""priority"": ""Blocker"",
      ""description"": ""We want to utilize the new rebalance protocol to mitigate the stop-the-world effect during the rebalance as our tasks are long running task.\r\n\r\nBut after the upgrade when we try to kill an instance to let rebalance happen when there is some load(some are long running tasks >30S) there, the CPU will go sky-high. It reads ~700% in our metrics so there should be several threads are in a tight loop. We have several consumer threads consuming from different partitions during the rebalance. This is reproducible in both the new\u00a0CooperativeStickyAssignor and old eager rebalance rebalance protocol. The difference is that with old\u00a0eager rebalance rebalance protocol used the high CPU usage will dropped after the rebalance done. But when using cooperative one, it seems the consumers threads are stuck on something and couldn't finish the rebalance so the high CPU usage won't drop until we stopped our load. Also a small load without long running task also won't cause continuous high CPU usage as the rebalance can finish in that case.\r\n\r\n\u00a0\r\n\r\n\""executor.kafka-consumer-executor-4\"" #124 daemon prio=5 os_prio=0 cpu=76853.07ms elapsed=841.16s tid=0x00007fe11f044000 nid=0x1f4 runnable\u00a0 [0x00007fe119aab000]\""executor.kafka-consumer-executor-4\"" #124 daemon prio=5 os_prio=0 cpu=76853.07ms elapsed=841.16s tid=0x00007fe11f044000 nid=0x1f4 runnable\u00a0 [0x00007fe119aab000]\u00a0 \u00a0java.lang.Thread.State: RUNNABLE at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:467) at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) at\r\n\r\n\u00a0\r\n\r\nBy debugging into the code we found it looks like the clients are\u00a0 in a loop on finding the coordinator.\r\n\r\nI also tried the old rebalance protocol for the new version the issue still exists but the CPU will be back to normal when the rebalance is done.\r\n\r\nAlso tried the same on the 2.4.1 which seems don't have this issue. So it seems related something changed between 2.4.1 and 2.5.0.\r\n\r\n\u00a0""
    },
    {
      ""issue_key"": ""KAFKA-9823"",
      ""title"": ""Consumer should check equality of the generation for coordinator requests"",
      ""priority"": ""Major"",
      ""description"": ""In consumer's requests to group coordinator (heartbeat, join-group, sync-group, commit; leave-group is not considered since consumer do not check its responses anyways), we encoded the generation / member information and the response may indicate that the member.id is invalid or generation is stale, which would cause consumer to reset and re-join group.\r\n\r\nHowever, when the response is sent back it is possible that the consumer has already started re-join due to other channels indicating it out of the group, and hence resetting would introduce unnecessarily more re-joining operations.\r\n\r\nWe should, instead, remember the generation information that was sent along with the request and upon getting the response compare that with the current generation information. If they do not match it means the responded error indicating stale information has been updated in other places and hence can be handled differently (for example, in heartbeat handling we can just ignore the error).""
    },
    {
      ""issue_key"": ""KAFKA-13406"",
      ""title"": ""Cooperative sticky assignor got stuck due to assignment validation failed"",
      ""priority"": ""Major"",
      ""description"": ""We'll do validateCooperativeAssignment for cooperative assignor, where we validate if there are previously owned partitions directly transfer to other consumers without \""revoke\"" step. However, the \""ownedPartition\"" in subscription might contain out-of-dated data, which might cause the validation always failure.\r\n\r\nWe should consider the short-term fix it by disabling validateCooperationAssignment for built-in cooperativeStickyAssignor because we've already consider the generation in the assignor, and discard the old generation ownedPartition if any.""
    },
    {
      ""issue_key"": ""KAFKA-12983"",
      ""title"": ""onJoinPrepare is not always invoked before joining the group"",
      ""priority"": ""Blocker"",
      ""description"": ""As the title suggests, the #onJoinPrepare callback is not always invoked before a member (re)joins the group, but only once when it first enters the rebalance. This means that any updates or events that occur during the join phase can be lost in the internal state: for example, clearing the SubscriptionState (and thus the \""ownedPartitions\"" that are used for cooperative rebalancing) after losing its memberId during a rebalance.\r\n\r\nWe should reset the `needsJoinPrepare` flag inside the resetStateAndRejoin() method""
    },
    {
      ""issue_key"": ""KAFKA-7312"",
      ""title"": ""Transient failure in kafka.api.AdminClientIntegrationTest.testMinimumRequestTimeouts"",
      ""priority"": ""Critical"",
      ""description"": ""{code}\r\nError Message\r\norg.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds\r\nStacktrace\r\norg.junit.runners.model.TestTimedOutException: test timed out after 120000 milliseconds\r\n\tat java.lang.Object.wait(Native Method)\r\n\tat java.lang.Object.wait(Object.java:502)\r\n\tat org.apache.kafka.common.internals.KafkaFutureImpl$SingleWaiter.await(KafkaFutureImpl.java:92)\r\n\tat org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:262)\r\n\tat kafka.utils.TestUtils$.assertFutureExceptionTypeEquals(TestUtils.scala:1345)\r\n\tat kafka.api.AdminClientIntegrationTest.testMinimumRequestTimeouts(AdminClientIntegrationTest.scala:1080)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)\r\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{code}""
    },
    {
      ""issue_key"": ""KAFKA-9620"",
      ""title"": ""Task revocation failure could introduce remaining unclean tasks"",
      ""priority"": ""Major"",
      ""description"": ""The task revocation call should enforce the close of a task, otherwise we could potentially hit the exception during `handleAssignment`.\r\n\r\nDuring revoke we failed:\r\n\r\n\u00a0\r\n{code:java}\r\n[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) [2020-02-27 19:05:47,321] ERROR [stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1] [Consumer clientId=stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1-consumer, groupId=stream-soak-test] User provided listener org.apache.kafka.streams.processor.internals.StreamsRebalanceListener failed on invocation of onPartitionsRevoked for partitions [logs.json.kafka-2, logs.json.zookeeper-2, node-name-repartition-1, logs.kubernetes-2, windowed-node-counts-1, logs.operator-2, logs.syslog-2] (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)\r\n[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) org.apache.kafka.streams.errors.TaskMigratedException: Producer get fenced trying to commit a transaction; it means all tasks belonging to this thread should be migrated.\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamsProducer.commitTransaction(StreamsProducer.java:172)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.commit(RecordCollectorImpl.java:226)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.commitState(StreamTask.java:368)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:242)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.handleRevocation(TaskManager.java:314)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:72)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:297)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:383)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:439)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:358)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:477)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1277)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1243)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1218)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:920)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:800)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725)\r\n[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.\r\n{code}\r\nDuring assignment we are checking the cleanness of task close and throw fatal:\r\n{code:java}\r\n[2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) [2020-02-27 19:05:48,032] ERROR [stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1] stream-thread [stream-soak-test-d1c291a8-ee54-4058-ac9c-7cd46d5484de-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:\u00a0 (org.apache.kafka.streams.processor.internals.StreamThread) [2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) java.lang.RuntimeException: Unexpected failure to close 1 task(s) [[0_2]]. First exception (for task 0_2) follows. \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:205) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1176) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:397) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:439) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:358) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:477) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1277) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1243) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1218) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:920) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:800) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:749) \u00a0 \u00a0 \u00a0 \u00a0 at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:725) [2020-02-27T11:05:48-08:00] (streams-soak-trunk-eos_soak_i-099dc04bf946ce2f0_streamslog) Caused by: org.apache.kafka.streams.errors.TaskMigratedException: Producer get fenced trying to commit a transaction; it means all tasks belonging to this thread should be migrated.\r\n{code}\r\n\u00a0""
    },
    {
      ""issue_key"": ""KAFKA-8104"",
      ""title"": ""Consumer cannot rejoin to the group after rebalancing"",
      ""priority"": ""Critical"",
      ""description"": ""TL;DR; {{KafkaConsumer}} cannot rejoin to the group due to inconsistent {{AbstractCoordinator.generation}} (which is {{NO_GENERATION}} and {{AbstractCoordinator.joinFuture}} (which is succeeded {{RequestFuture}}). See explanation below.\r\n\r\nThere are 16 consumers in single process (threads from pool-4-thread-1 to pool-4-thread-16). All of them belong to single consumer group {{hercules.sink.elastic.legacy_logs_elk_c2}}. Rebalancing has been acquired and consumers have got {{CommitFailedException}} as expected:\r\n\r\n{noformat}\r\n2019-03-10T03:16:37.023Z [pool-4-thread-10] WARN  r.k.vostok.hercules.sink.SimpleSink - Commit failed due to rebalancing\r\norg.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.\r\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:798)\r\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:681)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1334)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1298)\r\n\tat ru.kontur.vostok.hercules.sink.Sink.commit(Sink.java:156)\r\n\tat ru.kontur.vostok.hercules.sink.SimpleSink.run(SimpleSink.java:104)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n{noformat}\r\n\r\nAfter that, most of them successfully rejoined to the group with generation 10699:\r\n{noformat}\r\n2019-03-10T03:16:39.208Z [pool-4-thread-13] INFO  o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-13, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Successfully joined group with generation 10699\r\n2019-03-10T03:16:39.209Z [pool-4-thread-13] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-13, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Setting newly assigned partitions [legacy_logs_elk_c2-18]\r\n...\r\n2019-03-10T03:16:39.216Z [pool-4-thread-11] INFO  o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-11, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Successfully joined group with generation 10699\r\n2019-03-10T03:16:39.217Z [pool-4-thread-11] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-11, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Setting newly assigned partitions [legacy_logs_elk_c2-10, legacy_logs_elk_c2-11]\r\n...\r\n2019-03-10T03:16:39.218Z [pool-4-thread-15] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-15, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Setting newly assigned partitions [legacy_logs_elk_c2-24]\r\n2019-03-10T03:16:42.320Z [kafka-coordinator-heartbeat-thread | hercules.sink.elastic.legacy_logs_elk_c2] INFO  o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-6, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Attempt to heartbeat failed since group is rebalancing\r\n2019-03-10T03:16:42.320Z [kafka-coordinator-heartbeat-thread | hercules.sink.elastic.legacy_logs_elk_c2] INFO  o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-5, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Attempt to heartbeat failed since group is rebalancing\r\n2019-03-10T03:16:42.323Z [kafka-coordinator-heartbeat-thread | hercules.sink.elastic.legacy_logs_elk_c2] INFO  o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-7, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Attempt to heartbeat failed since group is rebalancing\r\n2019-03-10T03:17:13.235Z [pool-4-thread-4] INFO  o.a.k.c.c.i.AbstractCoordinator - [Consumer clientId=consumer-4, groupId=hercules.sink.elastic.legacy_logs_elk_c2] Successfully joined group with generation -1\r\n\r\n{noformat}\r\n\r\n\r\nBut one consumer (pool-4-thread-4) got strange generation -1 (see last log record from above).\r\nFurther log records in attached log file.\r\n\r\nFinally, 15 consumers successfully rejoined. But consumer with thread {{pool-4-thread-4}} didn't rejoin:\r\n\r\n{noformat}\r\n2019-03-10T03:17:13.355Z [pool-4-thread-4] ERROR r.k.vostok.hercules.sink.SimpleSink - Unspecified exception has been acquired\r\njava.lang.IllegalStateException: Coordinator selected invalid assignment protocol: null\r\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:241)\r\n\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:422)\r\n\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:352)\r\n\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:337)\r\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:333)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1218)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1175)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1154)\r\n\tat ru.kontur.vostok.hercules.sink.Sink.poll(Sink.java:152)\r\n\tat ru.kontur.vostok.hercules.sink.SimpleSink.run(SimpleSink.java:70)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n2019-03-10T03:17:13.360Z [pool-4-thread-4] ERROR r.k.vostok.hercules.sink.SimpleSink - Unspecified exception has been acquired\r\njava.lang.IllegalStateException: Coordinator selected invalid assignment protocol: null\r\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:241)\r\n\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:422)\r\n\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:352)\r\n\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:337)\r\n\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:333)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1218)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1175)\r\n\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1154)\r\n\tat ru.kontur.vostok.hercules.sink.Sink.poll(Sink.java:152)\r\n\tat ru.kontur.vostok.hercules.sink.SimpleSink.run(SimpleSink.java:70)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)}}\r\n{noformat}\r\n\r\nIt is important to note, that {{KafkaConsumer.coordinator.joinFuture}} is not null and succeeded, but {{ConsumerCoordinator}} cannot perform {{resetJoinGroupFuture()}} due to exception was thrown from {{onJoinComplete()}}:\r\n{code:java}\r\n            if (future.succeeded()) {\r\n                // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried.\r\n                ByteBuffer memberAssignment = future.value().duplicate();\r\n                onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment);\r\n\r\n                // We reset the join group future only after the completion callback returns. This ensures\r\n                // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded.\r\n                resetJoinGroupFuture();\r\n                needsJoinPrepare = true;\r\n            }\r\n{code}\r\n\r\nIf I understood correctly, the generation was changed to {{NO_GENERATION}} in another thread by one of CoordinatorResponseHandlers.\r\n \r\n [^consumer-rejoin-fail.log] ""
    },
    {
      ""issue_key"": ""KAFKA-13310"",
      ""title"": ""KafkaConsumer cannot jump out of the poll method, and the consumer is blocked in the ConsumerCoordinator method maybeAutoCommitOffsetsSync(Timer timer). Cpu and traffic of  Broker's side increase sharply"",
      ""priority"": ""Major"",
      ""description"": ""h2. Foreword\r\n\r\n\u00a0 \u00a0 \u00a0 Because our consumers' consumption logic is sometimes heavier, we refer to the configuration of Kafka stream [https://kafka.apache.org/documentation/#upgrade_10201_notable]\r\n Set max.poll.interval.ms to Integer.MAX_VALUE\r\n Our consumers have adopted method : consumer.subscribe(Pattern.compile(\"".*riven.*\""));\r\n\r\n\u00a0\r\nh2. Recurrence of the problem scene\r\n\r\noperate steps are\r\n (1) Test environment Kafka cluster: three brokers\r\n (2) Topics conforming to regular expressions include rivenTest1, rivenTest2, and rivenTest88\r\n (3) Only one consumer is needed, group.id is \""rivenReassign\"", consumer.subscribe(Pattern.compile(\"".*riven.*\""));\r\n (4) At the beginning, the group status is stable, and everything is normal for consumers, then I delete topic: rivenTest88\r\n\r\n\u00a0\r\nh2. Phenomenon\r\n\r\n\u00a0 \u00a0 \u00a0 Problem phenomenon\r\n \u00a0(1) The consumer is blocked in the poll method, no longer consume any messages, and the consumer log is always printing\r\n [main] WARN org.apache.kafka.clients.consumer.internals.ConsumerCoordinator-[Consumer clientId=consumer-rivenReassign-1, groupId=rivenReassign] Offset commit failed on partition rivenTest88-1 at offset 0: This server does not host this topic-partition.\r\n (2) The describe consumerGroup interface of Adminclient\u00a0 has always timed out, and the group status is no longer stable\r\n (3) The cpu and traffic of the broker are *significantly increased*\r\n\r\n\u00a0\r\n\r\n\u00a0\r\nh2. Problem tracking\r\n\r\n\u00a0 \u00a0By analyzing the kafkaConsumer code, the version is 2.8.1.\r\n I found that you introduced the waitForJoinGroup variable in the updateAssignmentMetadataIfNeeded method. For the reason, I attached the comment on the method: \""try to update assignment metadata BUT do not need to block on the timer for join group\"". See as below:\r\n\r\n\u00a0\r\n{code:java}\r\n if (includeMetadataInTimeout) {\r\n    // try to update assignment metadata BUT do not need to block on the timer for join group\r\n    updateAssignmentMetadataIfNeeded(timer, false);\r\n} else {\r\n    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) {\r\n        log.warn(\""Still waiting for metadata\"");\r\n    }\r\n}{code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\nBy tracing the code back layer by layer, it is found that the function of this variable is to construct a time.timer(0L) and pass it back to the method joinGroupIfNeeded (final Timer timer) in AbstractCoordinator. See as below:\r\n{code:java}\r\n// if not wait for join group, we would just use a timer of 0\r\n      if (!ensureActiveGroup(waitForJoinGroup ? timer : time.timer(0L))) {\r\n// since we may use a different timer in the callee, we'd still need \r\n// to update the original timer's current time after the call \r\n      timer.update(time.milliseconds()); \r\n      return false; \r\n}\r\n{code}\r\n\u00a0But you will find that there is a submethod onJoinPrepare in the method stack of joinGroupIfNeeded, and then there is a line of code in the onJoinPrepare method\r\n maybeAutoCommitOffsetsSync(time.timer(rebalanceConfig.rebalanceTimeoutMs)), the value of rebalanceConfig.rebalanceTimeoutMs is actually max.poll.interval.ms.\r\n Finally, I tracked down ConsumerCoordinator's method commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer)\r\n The input parameter offsets is subscriptions.allConsumed(), when I delete the topic: rivenTest88, commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer) method will *fall into an infinite loop! !*\r\n{code:java}\r\npublic boolean commitOffsetsSync(Map<TopicPartition, OffsetAndMetadata> offsets, Timer timer) {\r\n invokeCompletedOffsetCommitCallbacks();\r\n\r\n if (offsets.isEmpty())\r\n return true;\r\n\r\n do {\r\n if (coordinatorUnknown() && !ensureCoordinatorReady(timer)) {\r\n return false;\r\n }\r\n\r\n RequestFuture<Void> future = sendOffsetCommitRequest(offsets);\r\n client.poll(future, timer);\r\n\r\n // We may have had in-flight offset commits when the synchronous commit began. If so, ensure that\r\n // the corresponding callbacks are invoked prior to returning in order to preserve the order that\r\n // the offset commits were applied.\r\n invokeCompletedOffsetCommitCallbacks();\r\n\r\n if (future.succeeded()) {\r\n if (interceptors != null)\r\n interceptors.onCommit(offsets);\r\n return true;\r\n }\r\n\r\n if (future.failed() && !future.isRetriable())\r\n throw future.exception();\r\n\r\n timer.sleep(rebalanceConfig.retryBackoffMs);\r\n } while (timer.notExpired());\r\n\r\n return false;\r\n}{code}\r\n\u00a0\r\n\r\n\u00a0\r\n\r\n*The reason for the endless loop is:*\r\n (1) The expiration time of the timer is too long, which is max.poll.interval.ms\r\n (2) The offsets to be submitted contain dirty data and TopicPartition that no longer exists\r\n (3) The response future of sendOffsetCommitRequest(final Map<TopicPartition, OffsetAndMetadata> offsets) has always failed, and the exception in the future is UnknownTopicOrPartitionException. This exception is allowed to be retried.\r\n\r\nThen since the infinite loop interval above is 100ms by default, timer.sleep(rebalanceConfig.retryBackoffMs);\r\n If a large number of consumers have this problem at the same time, a large number of network requests will be generated to the Kafka broker, *resulting in a sharp increase in the cpu and traffic of the broker machine!*\r\n\r\n\u00a0\r\n\r\n\u00a0\r\nh2. Suggest\r\n\r\n1.maybeAutoCommitOffsetsSync(time.timer(rebalanceConfig.rebalanceTimeoutMs)), the time of this method is recommended not to use max.poll.interval.ms,\r\n This parameter is open to users to configure. Through the explanation of this parameter on the official website, I would never think that this parameter will be used in this place. At the same time, it will block KafkaConsumer's poll (final Duration timeout), even if I set consumer.poll (Duration.ofMillis(1000)).\r\n 2. In fact, in the poll (Timer timer, boolean waitForJoinGroup) method of ConsumerCoordinatord, before calling the ensureActiveGroup method, the consumer ensures that the local metadata is up to date, see the code\r\n\r\n\u00a0\r\n{code:java}\r\nif (rejoinNeededOrPending()) {\r\n    // due to a race condition between the initial metadata fetch and the initial rebalance,\r\n    // we need to ensure that the metadata is fresh before joining initially. This ensures\r\n    // that we have matched the pattern against the cluster's topics at least once before joining.\r\n    if (subscriptions.hasPatternSubscription()) {\r\n        // For consumer group that uses pattern-based subscription, after a topic is created,\r\n        // any consumer that discovers the topic after metadata refresh can trigger rebalance\r\n        // across the entire consumer group. Multiple rebalances can be triggered after one topic\r\n        // creation if consumers refresh metadata at vastly different times. We can significantly\r\n        // reduce the number of rebalances caused by single topic creation by asking consumer to\r\n        // refresh metadata before re-joining the group as long as the refresh backoff time has\r\n        // passed.\r\n        if (this.metadata.timeToAllowUpdate(time.milliseconds()) == 0) {\r\n            this.metadata.requestUpdate();\r\n        }\r\n\r\n        if (!client.ensureFreshMetadata(timer)) {\r\n            return false;\r\n        }\r\n    }\r\n\r\n    if (!ensureActiveGroup(timer)) {\r\n        return false;\r\n    }\r\n}\r\n{code}\r\n\u00a0\r\n\r\nThat is to say, the consumer knows which topic/topicPartition is legal before onJoinPrepare. In this case, why didn't you find the UnknownTopicOrPartitionException in the commitOffsetsSync method mentioned above,do not put the submitted offsets and the latest local metadata together for analysis, remove the non-existent topicpartitions, and then try to submit the offsets again. I think I can break out of the infinite loop by doing this\r\n\r\n3. Why must the offset be submitted synchronously in the onJoinPrepare method? Can't the offset be submitted asynchronously? Or provide a parameter for the user to choose whether to submit synchronously or asynchronously. Or provide a new parameter to control the maximum number of retries for synchronous submission here, instead of using the Timer constructed by max.poll.interval.ms.\r\n And if you don\u2019t really submit the offset here, it will not have much impact. It may cause repeated consumption of some messages. I still suggest to provide a new parameter to control whether you need to submit the offset.""
    },
    {
      ""issue_key"": ""KAFKA-13563"",
      ""title"": ""FindCoordinatorFuture never get cleared in non-group mode( consumer#assign)"",
      ""priority"": ""Major"",
      ""description"": ""In KAFKA-10793, we fix the race condition when lookup coordinator by clearing the _findCoordinatorFuture_ when handling the result, rather than in the listener callbacks. It works well under consumer group mode (i.e. Consumer#subscribe), but we found when user is using non consumer group mode (i.e. Consumer#assign) with group id provided (for offset commitment, so that there will be consumerCoordinator created), the _findCoordinatorFuture_ will never be cleared in some situations, and cause the offset committing keeps getting NOT_COORDINATOR error.\r\n\r\n\u00a0\r\n\r\nAfter KAFKA-10793, we clear the _findCoordinatorFuture_ in 2 places:\r\n # heartbeat thread\r\n # AbstractCoordinator#ensureCoordinatorReady\r\n\r\nBut in non consumer group mode with group id provided, there will be no (1)heartbeat thread , and it only call (2)AbstractCoordinator#ensureCoordinatorReady when 1st time consumer wants to fetch committed offset position. That is, after 2nd lookupCoordinator call, we have no chance to clear the _findCoordinatorFuture_ .\r\n\r\n\u00a0\r\n\r\nTo avoid the race condition as KAFKA-10793 mentioned, it's not safe to clear the _findCoordinatorFuture_ in the future listener. So, I think we can fix this issue by calling AbstractCoordinator#ensureCoordinatorReady when coordinator unknown in non consumer group case, under each Consumer#poll.\r\n\r\n\u00a0\r\n\r\nReproduce steps:\r\n\u00a0\r\n1. Start a 3 Broker cluster with a Topic having Replicas=3.\r\n2. Start a Client with Producer and Consumer (with Consumer#assign(), not subscribe, and provide a group id) communicating over the Topic.\r\n3. Stop the Broker that is acting as the\u00a0Group Coordinator.\r\n4. Observe successful Rediscovery of new Group Coordinator.\r\n5. Restart the stopped Broker.\r\n6. Stop the Broker that became the new Group Coordinator at step 4.\r\n7. Observe \""Rediscovery will be attempted\"" message but no \""Discovered group coordinator\"" message.\r\n\u00a0\r\n\u00a0\r\n\r\n\u00a0""
    },
    {
      ""issue_key"": ""KAFKA-8972"",
      ""title"": ""KafkaConsumer.unsubscribe could leave inconsistent user rebalance callback state"",
      ""priority"": ""Blocker"",
      ""description"": ""Our current implementation ordering of {{KafkaConsumer.unsubscribe}} is the following:\r\n\r\n{code}\r\nthis.subscriptions.unsubscribe();\r\nthis.coordinator.onLeavePrepare();\r\nthis.coordinator.maybeLeaveGroup(\""the consumer unsubscribed from all topics\"");\r\n{code}\r\n\r\nAnd inside {{onLeavePrepare}} we would look into the assignment and try to revoke them and notify users via {{RebalanceListener#onPartitionsRevoked}}, and then clear the assignment.\r\n\r\nHowever, the subscription's assignment is already cleared in {{this.subscriptions.unsubscribe();}} which means user's rebalance listener would never be triggered. In other words, from consumer client's pov nothing is owned after unsubscribe, but from the user caller's pov the partitions are not revoked yet. For callers like Kafka Streams which rely on the rebalance listener to maintain their internal state, this leads to inconsistent state management and failure cases.\r\n\r\nBefore KIP-429 this issue is hidden away since every time the consumer re-joins the group later, it would still revoke everything anyways regardless of the passed-in parameters of the rebalance listener; with KIP-429 this is easier to reproduce now.\r\n\r\nI think we can summarize our fix as:\r\n\r\n\u2022 Inside `unsubscribe`, first do `onLeavePrepare / maybeLeaveGroup` and then `subscription.unsubscribe`. This we we are guaranteed that the streams' tasks are all closed as revoked by then.\r\n\u2022 [Optimization] If the generation is reset due to fatal error from join / hb response etc, then we know that all partitions are lost, and we should not trigger `onPartitionRevoked`, but instead just `onPartitionsLost` inside `onLeavePrepare`.""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-8179: Part 3, Add PartitionsLost API for resetGenerations and metadata/subscription change"",
    ""pr_description"": ""Add onPartitionsLost into the RebalanceListener, which will be triggered when a) reset generations due to errors, and b) topic metadata changed and owned partitions no longer exist.\n\nWhen resetting generations proactively and subscription changes, we will trigger onPartitionsRevoked instead.\n\nSemantical behavior change: with COOPERATIVE protocol, if the revoked / lost partitions are empty, do not trigger the corresponding callback at all. For added partitions though, even if it is empty we would still trigger the callback as a way to notify the rebalance event; with EAGER protocol, revoked / assigned callbacks are always triggered.\n\nThe ordering of the callback would be the following:\na. Callback onPartitionsRevoked / onPartitionsLost triggered.\nb. Update the assignment (both revoked and added).\nc. Callback onPartitionsAssigned triggered.\nIn this way we are assured that users can still access the partitions being revoked, whereas they can also access the partitions being added.\n\n\nSemantical behavior change (KAFKA-4600): if the rebalance listener throws an exception, pass it along all the way to the consumer.poll caller, but still completes the rest of the actions. Also, the newly assigned partitions list does not gets affected with exception thrown since it is just for notifying the users.\n\n\nSemantical behavior change: the ConsumerCoordinator would not try to modify assignor's returned assignments, instead it will validate that assignments and set the error code accordingly: if there are overlaps between added / revoked partitions, it is a fatal error and would be communicated to all members to stop; if revoked is not empty, it is an error indicate re-join; otherwise, it is normal.\n\n\nMinor: with the error code removed from the Assignment, ConsumerCoordinator will request re-join if the revoked partitions list is not empty.\n\n\nUpdated ConsumerCoordinatorTest accordingly. Also found a minor bug in MetadataUpdate that removed topic would still be retained with null value of num.partitions.\n\n\nUpdated a few other unit tests that are exposed due to this change.\n\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",[],[],"{""issue_keys"": [""KAFKA-13709"", ""KAFKA-13563"", ""KAFKA-8972""], ""issues_details"": [{""issue_key"": ""KAFKA-13709"", ""title"": ""KafkaConsumer.poll() throws java.util.concurrent.TimeoutException when poll timeout is reached and there are no new records available"", ""priority"": ""Major"", ""description"": ""When using KafkaConsumer.poll() with a specified maximum poll time, if no new records are available within the specified time, the method should return an empty list, but instead it throws java.util.concurrent.TimeoutException.\n\nThis issue affects applications that rely on KafkaConsumer to process messages in a timely manner and could lead to unexpected behavior and application crashes.""}, {""issue_key"": ""KAFKA-13563"", ""title"": ""FindCoordinatorFuture never get cleared in non-group mode( consumer#assign)"", ""priority"": ""Major"", ""description"": ""In KAFKA-10793, we fix the race condition when lookup coordinator by clearing the _findCoordinatorFuture_ when handling the result, rather than in the listener callbacks. It works well under consumer group mode (i.e. Consumer#subscribe), but we found when user is using non consumer group mode (i.e. Consumer#assign) with group id provided (for offset commitment, so that there will be consumerCoordinator created), the _findCoordinatorFuture_ will never be cleared in some situations, and cause the offset committing keeps getting NOT_COORDINATOR error.\n\nAfter KAFKA-10793, we clear the _findCoordinatorFuture_ in 2 places:\n # heartbeat thread\n # AbstractCoordinator#ensureCoordinatorReady\n\nBut in non consumer group mode with group id provided, there will be no (1)heartbeat thread , and it only call (2)AbstractCoordinator#ensureCoordinatorReady when 1st time consumer wants to fetch committed offset position. That is, after 2nd lookupCoordinator call, we have no chance to clear the _findCoordinatorFuture_ .\n\nTo avoid the race condition as KAFKA-10793 mentioned, it's not safe to clear the _findCoordinatorFuture_ in the future listener. So, I think we can fix this issue by calling AbstractCoordinator#ensureCoordinatorReady when coordinator unknown in non consumer group case, under each Consumer#poll.""}, {""issue_key"": ""KAFKA-8972"", ""title"": ""KafkaConsumer.unsubscribe could leave inconsistent user rebalance callback state"", ""priority"": ""Blocker"", ""description"": ""Our current implementation ordering of {{KafkaConsumer.unsubscribe}} is the following:\n\r\n{code}\r\nthis.subscriptions.unsubscribe();\r\nthis.coordinator.onLeavePrepare();\r\nthis.coordinator.maybeLeaveGroup(\""the consumer unsubscribed from all topics\"");\r\n{code}\n\r\nAnd inside {{onLeavePrepare}} we would look into the assignment and try to revoke them and notify users via {{RebalanceListener#onPartitionsRevoked}}, and then clear the assignment.\n\r\nHowever, the subscription's assignment is already cleared in {{this.subscriptions.unsubscribe();}} which means user's rebalance listener would never be triggered. In other words, from consumer client's pov nothing is owned after unsubscribe, but from the user caller's pov the partitions are not revoked yet. For callers like Kafka Streams which rely on the rebalance listener to maintain their internal state, this leads to inconsistent state management and failure cases.\n\r\nBefore KIP-429 this issue is hidden away since every time the consumer re-joins the group later, it would still revoke everything anyways regardless of the passed-in parameters of the rebalance listener; with KIP-429 this is easier to reproduce now.""}], ""risky_pull_request_details"": {""pr_title"": ""KAFKA-8179: Part 3, Add PartitionsLost API for resetGenerations and metadata/subscription change"", ""pr_description"": ""Add onPartitionsLost into the RebalanceListener, which will be triggered when a) reset generations due to errors, and b) topic metadata changed and owned partitions no longer exist.\n\nWhen resetting generations proactively and subscription changes, we will trigger onPartitionsRevoked instead.\n\nSemantical behavior change: with COOPERATIVE protocol, if the revoked / lost partitions are empty, do not trigger the corresponding callback at all. For added partitions though, even if it is empty we would still trigger the callback as a way to notify the rebalance event; with EAGER protocol, revoked / assigned callbacks are always triggered.\n\nThe ordering of the callback would be the following:\na. Callback onPartitionsRevoked / onPartitionsLost triggered.\nb. Update the assignment (both revoked and added).\nc. Callback onPartitionsAssigned triggered.\nIn this way we are assured that users can still access the partitions being revoked, whereas they can also access the partitions being added.\n\n\nSemantical behavior change (KAFKA-4600): if the rebalance listener throws an exception, pass it along all the way to the consumer.poll caller, but still completes the rest of the actions. Also, the newly assigned partitions list does not gets affected with exception thrown since it is just for notifying the users.\n\n\nSemantical behavior change: the ConsumerCoordinator would not try to modify assignor's returned assignments, instead it will validate that assignments and set the error code accordingly: if there are overlaps between added / revoked partitions, it is a fatal error and would be communicated to all members to stop; if revoked is not empty, it is an error indicate re-join; otherwise, it is normal.\n\n\nMinor: with the error code removed from the Assignment, ConsumerCoordinator will request re-join if the revoked partitions list is not empty.\n\n\nUpdated ConsumerCoordinatorTest accordingly. Also found a minor bug in MetadataUpdate that removed topic would still be retained with null value of num.partitions.\n\n\nUpdated a few other unit tests that are exposed due to this change.""}, ""risk_type_labels"": [], ""explanations"": []}",,True
6785,Mistral,1769332265.8483264,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-9144"",
      ""title"": ""Early expiration of producer state can cause coordinator epoch to regress"",
      ""priority"": ""Major"",
      ""description"": ""Transaction markers are written by the transaction coordinator. In order to fence zombie coordinators, we use the leader epoch associated with the coordinator partition. Partition leaders verify the epoch in the WriteTxnMarker request and ensure that it can only increase. However, when producer state expires, we stop tracking the epoch and it is possible for monotonicity to be violated. Generally we expect expiration to be on the order of days, so it should be unlikely for this to be a problem.\r\n\r\nAt least that is the theory. We observed a case where a coordinator epoch decreased between nearly consecutive writes within a couple minutes of each other. Upon investigation, we found that producer state had been incorrectly expired. We believe the sequence of events is the following:\r\n # Producer writes transactional data and fails before committing\r\n # Coordinator times out the transaction and writes ABORT markers\r\n # Upon seeing the ABORT and the bumped epoch, the partition leader deletes state from the last epoch, which effectively resets the last timestamp for the producer to -1.\r\n # The coordinator becomes a zombie before getting a successful response and continues trying to send\r\n # The new coordinator notices the incomplete transaction and also sends markers\r\n # The partition leader accepts the write from the new coordinator\r\n # The producer state is expired because the last timestamp was -1\r\n # The partition leader accepts the write from the old coordinator\r\n\r\nBasically it takes an alignment of planets to hit this bug, but it is possible. If you hit it, then the broker may be unable to start because we validate epoch monotonicity during log recovery. The problem is in 3 when the timestamp gets reset. We should use the timestamp from the marker instead.\r\n\r\n\u00a0""
    },
    {
      ""issue_key"": ""KAFKA-9820"",
      ""title"": ""validateMessagesAndAssignOffsetsCompressed allocates batch iterator which is not used"",
      ""priority"": ""Major"",
      ""description"": ""KAFKA-8106 added a new skip key/value iterator that reduces allocations\u00a0[https://github.com/apache/kafka/commit/3e9d1c1411c5268de382f9dfcc95bdf66d0063a0].\r\n\r\nUnfortunately in LogValidator it creates that iterator but it never uses it, and this is quite expensive in terms of allocations.""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-8106: Skipping ByteBuffer allocation of key / value / headers in logValidator"",
    ""pr_description"": ""This is inspired by the original work of #6699 by @Flowermin. It tries to achieve the same CPU savings by reducing unnecessary byte allocation and corresponding GC. Unlike #6699 though, which depends on skipBytes of LZ4 which used a shared byte array, in this PR we create a skip buffer outside of the compressed input stream. The reason is that not all compressed inputstream's implementation is optimized, more specifically:\n\nGZIP used BufferedInputStream, which has a shared buffer, sized 16KB\nSNAPPY used its own SnappyInputStream -> InputStream, which dynamically allocate\nLZ4 used its own KafkaLZ4BlockInputStream, which has a shared buffer of 64KB\nZSTD used its own ZstdInputStream, but it's own overriden skip also dynamically allocate\n\nThe detailed implementation can be summarized as follows:\n\nAdd skipKeyValueIterator() into DefaultRecordBatch, used in LogValidator; also added PartialDefaultRecord which extends DefaultRecord.\n\n1.a. In order make this optimization really effective, we also need to refactor the LogValidator to refactor part of the validation per record into the outer loop so that we do not need to update inPlaceAssigment inside the loop any more. And then based on this boolean we can decide whether or not to use skipKeyValueIterator or not before the loop.\n1.b. Also used streaming iterator instead when skip-iterator cannot be used.\n\n\nWith SkipKeyValueIterator, pre-allocate a skip byte array with fixed size (2KB), and use this array to take the decompressed bytes through each record, validating metadata and key / value / header size, while skipping the key / value bytes.\n\n\nAlso tighten the unit tests of LogValidator to make sure scenarios like mismatched magic bytes / multiple batches per partition / discontinuous offsets / etc are indeed validated.\n\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}","['Bug Risk', 'Maintainability Risk']","[{'label': 'Bug Risk', 'confidence': 0.9, 'rationale': ""The SZZ origin issue 'KAFKA-9144' describes an observable functional failure where a coordinator epoch decreased between nearly consecutive writes, causing a regression in broker startup. This is a clear case of incorrect behavior.""}, {'label': 'Maintainability Risk', 'confidence': 0.8, 'rationale': ""The SZZ origin issue 'KAFKA-9820' describes the allocation of an iterator in LogValidator that is not used, leading to expensive allocations. This could potentially impact performance or maintainability due to increased memory usage and garbage collection.""}]","{""risk_type_labels"": [""Bug Risk"", ""Maintainability Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 0.9, ""rationale"": ""The SZZ origin issue 'KAFKA-9144' describes an observable functional failure where a coordinator epoch decreased between nearly consecutive writes, causing a regression in broker startup. This is a clear case of incorrect behavior.""}, {""label"": ""Maintainability Risk"", ""confidence"": 0.8, ""rationale"": ""The SZZ origin issue 'KAFKA-9820' describes the allocation of an iterator in LogValidator that is not used, leading to expensive allocations. This could potentially impact performance or maintainability due to increased memory usage and garbage collection.""}]}",,True
8692,Mistral,1769332271.263278,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-10018: Change command line tools from /bin/sh to /bin/bash"",
    ""pr_description"": ""\""#!/bin/sh\"" is used in kafka-server-stop.sh and zookeeper-server-stop.sh. [[ is a bash-builtin and used.\nModern Debian and Ubuntu systems, which symlink sh to dash by default. So \""[[: not found\"" will occur.\nChange \""#!/bin/sh\"" into \""#!/bin/bash\"" can avoid this error. Modify and make all scripts using bash.\nChange-Id: I733c6e31f76d768e71ac0e040a33da8f4bd8f005\nSigned-off-by: Jiamei Xie jiamei.xie@arm.com\nMore detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.\nSummary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",[],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No SZZ origin issues are reported in the given pull request details.'}]","{""risk_type_labels"": [], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No SZZ origin issues are reported in the given pull request details.""}], ""IMPORTANT"": ""The PR has no SZZ origin issues, hence it is 'Non-risky'.""}",,True
12127,Mistral,1769332276.9419844,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-13785: [8/N][emit final] time-ordered session store"",
    ""pr_description"": ""This is a copy PR of #11917. The major diffs are:\n\nAvoid extra byte array allocation for fixed upper/lower range serialization.\nRename some class names to be more consistent.\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'The pull request does not have any SZZ origin issues, and no observable failures are reported.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""The pull request does not have any SZZ origin issues, and no observable failures are reported.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
14085,Mistral,1769332282.5828583,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: fix docs markup"",
    ""pr_description"": ""Porting fixes from apache/kafka-site#528 back.\nThis should also be cherry-picked to 3.5 branch.""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'Since there are no SZZ origin issues linked to this pull request, it is considered non-risky.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""Since there are no SZZ origin issues linked to this pull request, it is considered non-risky.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
11756,Mistral,1769332289.6730664,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-13510"",
      ""title"": ""KIP-769: Connect APIs to list all connector plugins and retrieve their configuration definitions"",
      ""priority"": ""Major"",
      ""description"": """"
    },
    {
      ""issue_key"": ""KAFKA-15680"",
      ""title"": ""Partition-Count is not getting updated Correctly in the Incremental Co-operative Rebalancing(ICR) Mode of Rebalancing"",
      ""priority"": ""Minor"",
      ""description"": ""* In ICR(Incremental Cooperative Rebalancing) mode, whenever a new worker, say Worker 3 joins, a new global assignment is computed by the leader, say Worker1, that results in the revocation of some tasks from each existing worker i.e Worker1 and Worker2.\r\n * Once the new member join is completed, *ConsumerCoordinator.OnJoinComplete()* method is called which primarily computes all the new partitions assigned and the partitions which are revoked and updates the subscription Object.\r\n * If it was the case of revocation which we check by checking the \u201cpartitonsRevoked\u201d list, we call the method {*}\u201cinvoke{*}PartitionRevoked()\u201d which internally calls \u201cupdatePartitionCount()\u201d which fetches partition from the *assignment* object which is yet not updated by the new assignment.\r\n * It is only just before calling the \u201c{*}invokePartitionsAssigned{*}()\u201d method that we update the *assignment* by invoking the following \u2192 *subscriptions.assignFromSubscribed(assignedPartitions);*""
    },
    {
      ""issue_key"": ""KAFKA-9981"",
      ""title"": ""Running a dedicated mm2 cluster with more than one nodes,When the configuration is updated the task is not aware and will lose the update operation."",
      ""priority"": ""Major"",
      ""description"": ""DistributedHerder.reconfigureConnector\u00a0induction config update as follows:\r\n{code:java}\r\nif (changed) {\r\n    List<Map<String, String>> rawTaskProps = reverseTransform(connName, configState, taskProps);\r\n    if (isLeader()) {\r\n        configBackingStore.putTaskConfigs(connName, rawTaskProps);\r\n        cb.onCompletion(null, null);\r\n    } else {\r\n        // We cannot forward the request on the same thread because this reconfiguration can happen as a result of connector\r\n        // addition or removal. If we blocked waiting for the response from leader, we may be kicked out of the worker group.\r\n        forwardRequestExecutor.submit(new Runnable() {\r\n            @Override\r\n            public void run() {\r\n                try {\r\n                    String leaderUrl = leaderUrl();\r\n                    if (leaderUrl == null || leaderUrl.trim().isEmpty()) {\r\n                        cb.onCompletion(new ConnectException(\""Request to leader to \"" +\r\n                                \""reconfigure connector tasks failed \"" +\r\n                                \""because the URL of the leader's REST interface is empty!\""), null);\r\n                        return;\r\n                    }\r\n                    String reconfigUrl = RestServer.urlJoin(leaderUrl, \""/connectors/\"" + connName + \""/tasks\"");\r\n                    log.trace(\""Forwarding task configurations for connector {} to leader\"", connName);\r\n                    RestClient.httpRequest(reconfigUrl, \""POST\"", null, rawTaskProps, null, config, sessionKey, requestSignatureAlgorithm);\r\n                    cb.onCompletion(null, null);\r\n                } catch (ConnectException e) {\r\n                    log.error(\""Request to leader to reconfigure connector tasks failed\"", e);\r\n                    cb.onCompletion(e, null);\r\n                }\r\n            }\r\n        });\r\n    }\r\n}\r\n{code}\r\nKafkaConfigBackingStore task checks for configuration updates,such as topic whitelist update.If KafkaConfigBackingStore task is not running on leader node,an HTTP request will be send to notify the leader of the configuration update.However,dedicated mm2 cluster does not have the HTTP server turned on,so the request will fail to be sent,causing the update operation to be lost.""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""MINOR: Small cleanups in connect:runtime"",
    ""pr_description"": ""Committer Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Bug Risk'],"[{'label': 'Bug Risk', 'confidence': 0.9, 'rationale': 'The issue KAFKA-15680 describes a failure in the Incremental Cooperative Rebalancing (ICR) mode of rebalancing where the partition count is not updated correctly. This issue indicates an observable functional failure.'}]","{""risk_type_labels"": [""Bug Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 0.9, ""rationale"": ""The issue KAFKA-15680 describes a failure in the Incremental Cooperative Rebalancing (ICR) mode of rebalancing where the partition count is not updated correctly. This issue indicates an observable functional failure.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
6616,Mistral,1769332294.1836805,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-7895: fix Suppress changelog restore (#6536)"",
    ""pr_description"": ""Several issues have come to light since the 2.2.0 release:\nupon restore, suppress incorrectly set the record metadata using the changelog record, instead of preserving the original metadata\nrestoring a tombstone incorrectly didn't update the buffer size and min-timestamp\nCherry-picked from #6536 / 6538e9e\nReviewers: Guozhang Wang wangguoz@gmail.com, Matthias J. Sax mjsax@apache.org,  Bruno Cadonna bruno@confluent.io,  Bill Bejeck bbejeck@gmail.com\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],[],"{""risk_type_labels"": [""Non-risky""], ""explanations"": [], ""IMPORTANT"": ""The provided PR does not have any SZZ origin issues, thus it is classified as 'Non-risky'.""}",,True
7872,Mistral,1769332299.8118756,"{
  ""szz_origin_issues"": [],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-9337 simplify mm2 initial configs"",
    ""pr_description"": ""More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.\nSummary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Non-risky'],"[{'label': 'Non-risky', 'confidence': 1.0, 'rationale': 'No SZZ origin issues have been identified for this PR, and no observable failures attributed to it.'}]","{""risk_type_labels"": [""Non-risky""], ""explanations"": [{""label"": ""Non-risky"", ""confidence"": 1.0, ""rationale"": ""No SZZ origin issues have been identified for this PR, and no observable failures attributed to it.""}], ""IMPORTANT"": ""Return VALID JSON ONLY. The 'confidence' field MUST be a numeric value, not a string.""}",,True
14685,Mistral,1769332308.1549706,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-16226"",
      ""title"": ""Java client: Performance regression in Trogdor benchmark with high partition counts"",
      ""priority"": ""Major"",
      ""description"": ""h1. Background\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-15415 implemented optimisation in java-client to skip backoff period if client knows of a newer leader, for produce-batch being retried.\r\nh1. What changed\r\n\r\nThe implementation introduced a regression noticed on a trogdor-benchmark running with high partition counts(36000!).\r\nWith regression, following metrics changed on the produce side.\r\n # record-queue-time-avg: increased from 20ms to 30ms.\r\n # request-latency-avg: increased from 50ms to 100ms.\r\n\r\nh1. Why it happened\r\n\r\nAs can be seen from the original [PR|https://github.com/apache/kafka/pull/14384] RecordAccmulator.partitionReady() & drainBatchesForOneNode() started using synchronised method Metadata.currentLeader(). This has led to increased synchronization between KafkaProducer's application-thread that call send(), and background-thread that actively send producer-batches to leaders.\r\n\r\nLock profiles clearly show increased synchronisation in KAFKA-15415 PR(highlighted in {color:#de350b}Red{color}) Vs baseline ( see below ). Note the synchronisation is much worse for paritionReady() in this benchmark as its called for each partition, and it has 36k partitions!\r\nh3. Lock Profile: Kafka-15415\r\n\r\n!kafka_15415_lock_profile.png!\r\nh3. Lock Profile: Baseline\r\n\r\n!baseline_lock_profile.png!\r\nh1. Fix\r\n\r\nSynchronization has to be reduced between 2 threads in order to address this. [https://github.com/apache/kafka/pull/15323] is a fix for it, as it avoids using Metadata.currentLeader() instead rely on Cluster.leaderFor().\r\n\r\nWith the fix, lock-profile & metrics are similar to baseline.\r\n\r\n\u00a0""
    },
    {
      ""issue_key"": ""KAFKA-16012"",
      ""title"": ""Incomplete range assignment in consumer"",
      ""priority"": ""Blocker"",
      ""description"": ""We were looking into test failures here: https://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/system-test-kafka-branch-builder--1702475525--jolshan--kafka-15784--7cad567675/2023-12-13--001./2023-12-13\u2013001./report.html.\u00a0\r\n\r\nHere is the first failure in the report:\r\n{code:java}\r\n====================================================================================================\r\ntest_id:\u00a0 \u00a0 kafkatest.tests.core.group_mode_transactions_test.GroupModeTransactionsTest.test_transactions.failure_mode=clean_bounce.bounce_target=brokers\r\nstatus: \u00a0 \u00a0 FAIL\r\nrun time: \u00a0 3 minutes 4.950 seconds\r\n\r\n\r\n\u00a0 \u00a0 TimeoutError('Consumer consumed only 88223 out of 100000 messages in 90s') {code}\r\n\u00a0\r\n\r\nWe traced the failure to an apparent bug during the last rebalance before the group became empty. The last remaining instance seems to receive an incomplete assignment which prevents it from completing expected consumption on some partitions. Here is the rebalance from the coordinator's perspective:\r\n{code:java}\r\nserver.log.2023-12-13-04:[2023-12-13 04:58:56,987] INFO [GroupCoordinator 3]: Stabilized group grouped-transactions-test-consumer-group generation 5 (__consumer_offsets-2) with 1 members (kafka.coordinator.group.GroupCoordinator)\r\nserver.log.2023-12-13-04:[2023-12-13 04:58:56,990] INFO [GroupCoordinator 3]: Assignment received from leader consumer-grouped-transactions-test-consumer-group-1-2164f472-93f3-4176-af3f-23d4ed8b37fd for group grouped-transactions-test-consumer-group for generation 5. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator) {code}\r\nThe group is down to one member in generation 5. In the previous generation, the consumer in question reported this assignment:\r\n{code:java}\r\n// Gen 4: we've got partitions 0-4\r\n[2023-12-13 04:58:52,631] DEBUG [Consumer clientId=consumer-grouped-transactions-test-consumer-group-1, groupId=grouped-transactions-test-consumer-group] Executing onJoinComplete with generation 4 and memberId consumer-grouped-transactions-test-consumer-group-1-2164f472-93f3-4176-af3f-23d4ed8b37fd (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)\r\n[2023-12-13 04:58:52,631] INFO [Consumer clientId=consumer-grouped-transactions-test-consumer-group-1, groupId=grouped-transactions-test-consumer-group] Notifying assignor about the new Assignment(partitions=[input-topic-0, input-topic-1, input-topic-2, input-topic-3, input-topic-4]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) {code}\r\nHowever, in generation 5, we seem to be assigned only one partition:\r\n{code:java}\r\n// Gen 5: Now we have only partition 1? But aren't we the last member in the group?\r\n[2023-12-13 04:58:56,954] DEBUG [Consumer clientId=consumer-grouped-transactions-test-consumer-group-1, groupId=grouped-transactions-test-consumer-group] Executing onJoinComplete with generation 5 and memberId consumer-grouped-transactions-test-consumer-group-1-2164f472-93f3-4176-af3f-23d4ed8b37fd (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)\r\n[2023-12-13 04:58:56,955] INFO [Consumer clientId=consumer-grouped-transactions-test-consumer-group-1, groupId=grouped-transactions-test-consumer-group] Notifying assignor about the new Assignment(partitions=[input-topic-1]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) {code}\r\nThe assignment type is range from the JoinGroup for generation 5. The decoded metadata sent by the consumer is this:\r\n{code:java}\r\nSubscription(topics=[input-topic], ownedPartitions=[], groupInstanceId=null, generationId=4, rackId=null) {code}\r\nHere is the decoded assignment from the SyncGroup:\r\n{code:java}\r\nAssignment(partitions=[input-topic-1]) {code}\r\n\u00a0""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-15627: KIP-951's Leader discovery optimisations on the client"",
    ""pr_description"": ""NOTE - This PR was originally reviewed here , but was auto-closed due to github workflows, as explained here.\nThis implements the leader discovery optimisations for the client on KIP-951.\n\nOptimisation1: On discovering a new leader, produce-batch should skip any retry-backoff. This was implemented in KAFKA-15415\nOptimisation2: FetchResponse/ProduceResponse would return new leader info. This information is then used to update the Metadata cached.\nThis PR focuses on optimisation2 from above. Additionally it fixes a bug that got introduced to MetadataCache.java, details inline in a comment.\n\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Performance Risk'],"[{'label': 'Performance Risk', 'confidence': 0.9, 'rationale': 'In SZZ origin issue KAFKA-16226, the description mentions an observed performance regression in Trogdor benchmark with high partition counts, where record-queue-time-avg increased from 20ms to 30ms and request-latency-avg increased from 50ms to 100ms. This clearly indicates a performance degradation due to the PR KAFKA-15627.'}]","{""risk_type_labels"": [""Performance Risk""], ""explanations"": [{""label"": ""Performance Risk"", ""confidence"": 0.9, ""rationale"": ""In SZZ origin issue KAFKA-16226, the description mentions an observed performance regression in Trogdor benchmark with high partition counts, where record-queue-time-avg increased from 20ms to 30ms and request-latency-avg increased from 50ms to 100ms. This clearly indicates a performance degradation due to the PR KAFKA-15627.""}], ""IMPORTANT"": ""Valid JSON""}",,True
13749,Mistral,1769332313.8809283,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-9366"",
      ""title"": ""Upgrade log4j to log4j2"",
      ""priority"": ""Critical"",
      ""description"": ""h2. CVE-2019-17571\u00a0Detail\r\n\r\nIncluded in Log4j 1.2 is a SocketServer class that is vulnerable to deserialization of untrusted data which can be exploited to remotely execute arbitrary code when combined with a deserialization gadget when listening to untrusted network traffic for log data. This affects Log4j versions up to 1.2 up to 1.2.17.\r\n\r\n\u00a0\r\n\r\n[https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-17571]\r\n\r\n\u00a0""
    },
    {
      ""issue_key"": ""KAFKA-16322"",
      ""title"": ""Fix CVE-2023-50572 by updating jline from 3.22.0 to 3.25.1"",
      ""priority"": ""Major"",
      ""description"": ""https://devhub.checkmarx.com/cve-details/CVE-2023-50572/\r\n\r\n""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-15016: Update LICENSE-binary file"",
    ""pr_description"": ""The file was getting a bit out of sync with the actual dependencies we ship. Also the process we follow each release catches missing licenses but does not clear dependencies not used anymore, so this removes a few unnecessary entries too.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Security Risk'],"[{'label': 'Security Risk', 'confidence': 1.0, 'rationale': ""The SZZ origin issue 'KAFKA-9366' describes a vulnerability (CVE-2019-17571) related to remote code execution in Log4j, which is a clear example of a security risk.""}]","{""risk_type_labels"": [""Security Risk""], ""explanations"": [{""label"": ""Security Risk"", ""confidence"": 1.0, ""rationale"": ""The SZZ origin issue 'KAFKA-9366' describes a vulnerability (CVE-2019-17571) related to remote code execution in Log4j, which is a clear example of a security risk.""}]}",,True
11252,Mistral,1769332320.9783275,"{
  ""szz_origin_issues"": [
    {
      ""issue_key"": ""KAFKA-14460"",
      ""title"": ""In-memory store iterators can return results with null values"",
      ""priority"": ""Major"",
      ""description"": ""Due to the thread-safety model we adopted in our in-memory stores to avoid scaling issues, we synchronize all read/write methods and then during range scans, copy the keyset of all results rather than returning a direct iterator over the underlying map. When users call #next to read out the iterator results, we issue a point lookup on the next key and then simply return a new KeyValue<>(key, get(key))\r\n\r\nThis lets the range scan return results without blocking access to the store by other threads and without risk of ConcurrentModification, as a writer can modify the real store without affecting the keyset copy of the iterator. This also means that those changes won't be reflected in what the iterator sees or returns, which in itself is fine as we don't guarantee consistency semantics of any kind.\r\n\r\nHowever, we\u00a0_do_ guarantee that range scans \""must not return null values\"" \u2013 and this contract may be violated if the StreamThread deletes a record that the iterator was going to return.\r\n\r\ntl;dr we should check get(key) for null and skip to the next result if necessary in the in-memory store iterators. See for example InMemoryKeyValueIterator (note that we'll probably need to buffer one record in advance before we return true from #hasNext)""
    },
    {
      ""issue_key"": ""KAFKA-15417"",
      ""title"": ""JoinWindow does not  seem to work properly with a KStream - KStream - LeftJoin()"",
      ""priority"": ""Major"",
      ""description"": ""In Kafka-streams 3.4.0 :\r\n\r\nAccording to the javadoc of the Joinwindow:\r\n\r\n_There are three different window configuration supported:_\r\n * _before = after = time-difference_\r\n * _before = 0 and after = time-difference_\r\n * _*before = time-difference and after = 0*_\r\n\r\n\u00a0\r\n\r\nHowever if\u00a0I use a joinWindow with *before = time-difference and after = 0* \r\non a kstream-kstream-leftjoin the *after=0* part does not seem to work.\r\n\r\nWhen using _stream1.leftjoin(stream2, joinWindow)_ with {_}joinWindow{_}.{_}after=0 and joinWindow.before=30s{_}, any new message on stream 1 that can not be joined with any messages on stream2 should be joined with a null-record after the _joinWindow.after_ has ended and a new message has arrived on stream1.\r\n\r\nIt does not.\r\n\r\nOnly if the new message arrives after the value of _joinWindow.before_ the previous message will be joined with a null-record.\r\n\r\n\u00a0\r\n\r\nAttached you can find two files with a TopologyTestDriver Unit test to reproduce.\r\n\r\ntopology:\u00a0 \u00a0stream1.leftjoin( stream2, joiner, joinwindow)\r\n\r\njoinWindow has before=5000ms and after=0\r\n\r\nmessage1(key1) ->\u00a0 stream1\r\n\r\nafter 4000ms message2(key2) -> stream1\u00a0 ->\u00a0 NO null-record join was made, but the after period was expired.\r\nafter 4900ms message2(key2) -> stream1\u00a0 ->\u00a0 NO null-record join was made, but the after period was expired.\r\nafter 5000ms message2(key2) -> stream1\u00a0 ->\u00a0 A null-record join was made,\u00a0 before period was expired.\r\nafter 6000ms message2(key2) -> stream1\u00a0 ->\u00a0 A null-record join was made,\u00a0 before period was expired.\r\n\r\n\r\n\r\n\u00a0""
    }
  ],
  ""risky_pull_request_details"": {
    ""pr_title"": ""KAFKA-13216: Use a KV with list serde for the shared store"",
    ""pr_description"": ""This is an alternative approach in parallel to #11235. After several unsuccessful trials to improve its efficiency i've come up with a larger approach, which is to use a kv-store instead as the shared store, which would store the value as list. The benefits of this approach are:\n\nOnly serde once that compose <timestamp, byte, key>, at the outer metered stores, with less byte array copies.\nDeletes are straight-forward with no scan reads, just a single call to delete all duplicated <timestamp, byte, key> values.\nUsing a KV store has less space amplification than a segmented window store.\n\nThe cons though:\n\nEach put call would be a get-then-write to append to the list; also we would spend a few more bytes to store the list (most likely a singleton list, and hence just 4 more bytes).\nIt's more complicated definitely.. :)\n\nThe main idea is that since the shared store is actively GC'ed by the expiration logic, not based on time retention, and since that the key format is in <timestamp, byte, key>, the range expiration query is quite efficient as well.\nAdded testing covering for the list stores (since we are still use kv-store interface, we cannot leverage on the get() calls in the stream-stream join, instead we use putIfAbsent and range only). Another minor factoring piggy-backed is to let toList to always close iterator to avoid leaking.\nCommitter Checklist (excluded from commit message)\n\n Verify design and implementation\n Verify test coverage and CI build status\n Verify documentation (including upgrade notes)""
  }
}",['Bug Risk'],"[{'label': 'Bug Risk', 'confidence': 0.9, 'rationale': 'In the pull request KAFKA-13216, a change was made to use a KV store with list serde for the shared store. This modification results in each put call being a get-then-write to append to the list, which could potentially lead to functional or logical failures such as incorrect behavior, wrong outputs, crashes, exceptions, or broken features.'}]","{""risk_type_labels"": [""Bug Risk""], ""explanations"": [{""label"": ""Bug Risk"", ""confidence"": 0.9, ""rationale"": ""In the pull request KAFKA-13216, a change was made to use a KV store with list serde for the shared store. This modification results in each put call being a get-then-write to append to the list, which could potentially lead to functional or logical failures such as incorrect behavior, wrong outputs, crashes, exceptions, or broken features.""}]}",,True
